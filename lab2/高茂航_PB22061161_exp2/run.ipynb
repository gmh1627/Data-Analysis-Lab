{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('https://dblp.dagstuhl.de/db/conf/kdd/kdd2023.html') as response:\n",
    "    html = response.read()\n",
    "\n",
    "html_text = html.decode()\n",
    "\n",
    "with open('page.txt','w',encoding='utf-8') as f:\n",
    "    f.write(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Track Full Papers\n",
      "Applied Data Track Full Papers\n",
      "Hands On Tutorials\n",
      "Lecture Style Tutorials\n",
      "Workshop Summaries\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('page.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 使用正则表达式找到所有的 <h2 id=\"*\"> 和 </h2> 之间的字符串\n",
    "matches = re.findall(r'<h2 id=\".*?\">(.*?)</h2>', content)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of research papers:  313\n",
      "Number of applied papers:  183\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "with open('page.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 定义一个列表来存储 Track 信息\n",
    "tracks = []\n",
    "\n",
    "# 定义正则表达式\n",
    "track_pattern = re.compile(r'<h2 id=\".*?\">(.*?)</h2>')\n",
    "author_pattern = re.compile(r'<span itemprop=\"name\" title=\".*?\">(.*?)</span>')\n",
    "title_pattern = re.compile(r'<span class=\"title\" itemprop=\"name\">(.*?)</span>')\n",
    "page_pattern = re.compile(r'<span itemprop=\"pagination\">(.*?)-(.*?)</span>')\n",
    "\n",
    "# 找到 \"Research Track Full Papers\" 和 \"Applied Data Science Track Full Papers\" 的位置\n",
    "start1 = content.find('Research Track Full Papers') - 50\n",
    "start2 = content.find('Applied Data Track Full Papers') - 50\n",
    "start3 = content.find('Hands On Tutorials') - 1\n",
    "\n",
    "# 从整篇文本中划分出前两个Track中所有相邻\"<cite\"和\"</cite>\"之间的内容(即一篇文章的范围)\n",
    "research_papers_content = re.split('<cite', content[start1:start2])[1:]\n",
    "applied_papers_content = re.split('<cite', content[start2:start3])[1:]\n",
    "\n",
    "def extract_paper_info(papers_content):\n",
    "    papers = []\n",
    "    for paper_content in papers_content:\n",
    "        paper_content = re.split('</cite>', paper_content)[0]\n",
    "        papers.append(paper_content)\n",
    "    return papers\n",
    "        \n",
    "spit_research_content = extract_paper_info(research_papers_content)\n",
    "spit_applied_content = extract_paper_info(applied_papers_content)\n",
    "\n",
    "print(\"Number of research papers: \", len(research_papers_content))\n",
    "print(\"Number of applied papers: \", len(applied_papers_content))\n",
    "\n",
    "# 提取每篇paper的author、title和startPage, endPage\n",
    "def extract_paper_info(papers_content):\n",
    "    papers = []\n",
    "    for paper_content in papers_content:\n",
    "        authors = author_pattern.findall(paper_content)\n",
    "        titles = title_pattern.findall(paper_content)\n",
    "        pages = page_pattern.search(paper_content)\n",
    "        startPage, endPage = pages.groups()\n",
    "        papers.extend([{'authors': authors, 'title': title , 'startPage': startPage , 'endPage': endPage} for title in titles])\n",
    "    return papers\n",
    "\n",
    "# 提取 \"Research Track Full Papers\" 的论文信息\n",
    "research_track = track_pattern.search(content[start1:start2]).group(1)\n",
    "research_papers = extract_paper_info(spit_research_content)\n",
    "\n",
    "# 提取 \"Applied Data Science Track Full Papers\" 的论文信息\n",
    "applied_track = track_pattern.search(content[start2:start3]).group(1)\n",
    "#applied_papers = extract_paper_info(spit_applied_content)\n",
    "applied_papers = extract_paper_info(spit_applied_content)\n",
    "# 将论文信息存储到字典列表中\n",
    "tracks.append({'track': research_track, 'papers': research_papers})\n",
    "tracks.append({'track': applied_track, 'papers': applied_papers})\n",
    "\n",
    "# 将字典列表转换为 JSON 并写入文件\n",
    "with open('kdd23.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tracks, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 打开并读取 \"page.txt\" 文件\n",
    "with open('page.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 定义正则表达式\n",
    "author_link_pattern = re.compile(r'<span itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\"><a href=\"(.*?)\" itemprop=\"url\">')\n",
    "orcID_pattern = re.compile(r'<img alt=\"\" src=\"https://dblp.dagstuhl.de/img/orcid.*?.16x16.png\" class=\"icon\">(.{19})</a></li>')\n",
    "researcher_pattern = re.compile(r'<span class=\"name primary\" itemprop=\"name\">(.*?) <span class=\"homonym-nr\">')\n",
    "year_pattern = re.compile(r'<span itemprop=\"datePublished\">(.*?)</span>')\n",
    "\n",
    "# 找到 \"Research Track Full Papers\" 和 \"Applied Data Track Full Papers\" 的位置\n",
    "start1 = content.find('Research Track Full Papers')\n",
    "start2 = content.find('Applied Data Track Full Papers')\n",
    "end = len(content)\n",
    "\n",
    "# 提取这两个部分的内容，并找到前 10 个 \"persistent URL:\" 之间的内容\n",
    "research_papers_content = content[start1:start2].split('<cite')[1:11]\n",
    "applied_papers_content = content[start2:end].split('<cite')[1:11]\n",
    "\n",
    "def extract_paper_info(papers_content):\n",
    "    papers = []\n",
    "    for paper_content in papers_content:\n",
    "        paper_content = re.split('</cite>', paper_content)[0]\n",
    "        papers.append(paper_content)\n",
    "    return papers\n",
    "        \n",
    "spit_research_content = extract_paper_info(research_papers_content)\n",
    "spit_applied_content = extract_paper_info(applied_papers_content)\n",
    "\n",
    "def extract_paper_info2(paper_content):\n",
    "    final_result = []\n",
    "    # 使用正则表达式找到所有在 \"<>\" 之外的字符串\n",
    "    outside_brackets = re.split(r'<[^>]*>', paper_content)\n",
    "    # 遍历提取到的内容，删除含有'http'的字符串及其前面的字符串\n",
    "    flag = -1\n",
    "    for i in range(len(outside_brackets)):\n",
    "        if 'http' in outside_brackets[i]:\n",
    "            flag = i\n",
    "    for i in range(flag + 1 , len(outside_brackets)):\n",
    "        if outside_brackets[i]:\n",
    "            final_result.append(outside_brackets[i])\n",
    "    return final_result\n",
    "\n",
    "# 定义一个列表来存储研究者信息\n",
    "researchers = []\n",
    "\n",
    "# 访问每篇文章里所有作者的链接，获取作者的 orcID 和论文信息\n",
    "for papers in [research_papers_content, applied_papers_content]:\n",
    "    for paper in papers:\n",
    "        author_links = author_link_pattern.findall(paper)\n",
    "        for link in author_links:\n",
    "            link_content = requests.get(link)\n",
    "            response = link_content.text\n",
    "            \n",
    "            #爬虫时频繁请求服务器，可能会被网站认定为攻击行为并报错\"ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接\"，故采取以下两个措施\n",
    "            #使用完后关闭响应\n",
    "            link_content.close()  \n",
    "            # 在各个请求之间添加随机延时等待\n",
    "            time.sleep(random.randint(1, 3))\n",
    "            \n",
    "            tempmatch = researcher_pattern.search(response)\n",
    "            if tempmatch is not None:\n",
    "                researcher = tempmatch.group(1)\n",
    "            else:\n",
    "                researcher_pattern1 = re.compile(r'<span class=\"name primary\" itemprop=\"name\">(.*?)</span>')\n",
    "                researcher = researcher_pattern1.search(response).group(1)\n",
    "            orcID = orcID_pattern.findall(response)\n",
    "            \n",
    "            # 找到 \"<li class=\"underline\" title=\"jump to the 2020s\">\" 和 \"<li class=\"underline\" title=\"jump to the 2010s\">\" 之间的内容\n",
    "            start = response.find('2020 &#8211; today')\n",
    "            end = response.find('<header id=\"the2010s\" class=\"hide-head h2\">')\n",
    "            # 提取这部分的内容，并找到所有 \"</cite>\" 之间的内容\n",
    "            papers_content = response[start:end].split('</cite>')[0:-1]\n",
    "            \n",
    "            papers_dict = []\n",
    "            \n",
    "            for paper_content in papers_content:\n",
    "                spit_content = extract_paper_info2(paper_content)\n",
    "                year = int(year_pattern.search(paper_content).group(1))\n",
    "                authors = []\n",
    "                publishInfo = []\n",
    "                for i in range(0 , len(spit_content) - 1):\n",
    "                    if spit_content[i] != \", \" and (spit_content[i+1] == \", \" or spit_content[i+1] == \":\"):\n",
    "                        authors.append(spit_content[i])\n",
    "                    elif spit_content[i-2] == \":\" and spit_content[i-1] == \" \":\n",
    "                        title = spit_content[i]\n",
    "                        for k in range(i+2 , len(spit_content)):\n",
    "                            publishInfo.append(spit_content[k])\n",
    "                # 创建一个新的字典来存储每篇文章的信息\n",
    "                paper_dict = {'authors': authors, 'title': title, 'publishInfo': ''.join(publishInfo), 'year': year}\n",
    "                papers_dict.append(paper_dict)\n",
    "            researchers.append({'researcher': researcher, 'orcID': orcID, 'papers': papers_dict})\n",
    "\n",
    "# 将字典列表转换为 JSON 并写入 \"researchers.json\" 文件\n",
    "with open('researchers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(researchers, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
